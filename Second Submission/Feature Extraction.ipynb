{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Feature Extraction.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.1"}},"cells":[{"cell_type":"code","metadata":{"colab_type":"code","id":"hQYqPEyytT2a","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"4be73e4c-ac19-490e-9b39-0c937b7761b2","executionInfo":{"status":"ok","timestamp":1585709425172,"user_tz":300,"elapsed":2087,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}}},"source":["import numpy as np\n","import math\n","import os.path\n","import csv\n","import glob\n","import tensorflow as tf\n","import h5py as h5py\n","from keras.preprocessing import image\n","from keras.applications.inception_v3 import InceptionV3, preprocess_input\n","from keras.models import Model, load_model"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"gnMlYTy8alZ6","colab_type":"code","colab":{}},"source":["# Package for face landmark detection\n","# !pip install mlxtend\n","import mlxtend"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"L09UMHR3unrx","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"24ed9823-f128-44c9-c161-064d33aa558c","executionInfo":{"status":"ok","timestamp":1585709441140,"user_tz":300,"elapsed":17378,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}}},"source":["import mlxtend\n","import matplotlib.pyplot as plt\n","from mlxtend.image import extract_face_landmarks"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Downloading http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2 to /root/mlxtend_data/shape_predictor_68_face_landmarks.dat.bz2\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"UzBlYSVyyirb","colab_type":"text"},"source":["# New Section"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"b-JU_7-ytcqx","outputId":"d11e402f-4b9c-4360-9022-8c5b3c6cd6ef","executionInfo":{"status":"ok","timestamp":1585709472058,"user_tz":300,"elapsed":30888,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"execution_count":6,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive/\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"I1BN_yg-PlmU","outputId":"34083d90-01d7-4849-b805-06a6c4a9541b","executionInfo":{"status":"ok","timestamp":1585709477818,"user_tz":300,"elapsed":1631,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}},"colab":{"base_uri":"https://localhost:8080/","height":595}},"source":["! ls '/content/drive/My Drive/CSCE_689'\n"],"execution_count":7,"outputs":[{"output_type":"stream","text":[" 0.avi\t  40.avi\t\t\t\t     Labels_final.csv\n"," 10.avi   41.avi\t\t\t\t     model1.h5\n"," 11.avi   42.avi\t\t\t\t     model.h5\n"," 12.avi   43.avi\t\t\t\t     model.json\n"," 13.avi   44.avi\t\t\t\t     model.png\n"," 14.avi   45.avi\t\t\t\t     optimization\n"," 15.avi   46.avi\t\t\t\t     Prediction\n"," 16.avi   47.avi\t\t\t\t     README.md\n"," 17.avi   48.avi\t\t\t\t     Test\n"," 18.avi   49.avi\t\t\t\t    'Test10 - NN.png'\n"," 19.avi   4.avi\t\t\t\t\t    'Test10 - TL.png'\n"," 1.avi\t  50.avi\t\t\t\t     Test_1.avi\n"," 20.avi   51.avi\t\t\t\t    'Test1 - NN.png'\n"," 21.avi   52.avi\t\t\t\t     Test_2.avi\n"," 22.avi   53.avi\t\t\t\t    'Test2 - NN.png'\n"," 23.avi   54.avi\t\t\t\t     Test_3.avi\n"," 24.avi   55.avi\t\t\t\t    'Test3 - NN.png'\n"," 25.avi   56.avi\t\t\t\t     Test_4.avi\n"," 26.avi   57.avi\t\t\t\t    'Test4 - NN.png'\n"," 27.avi   58.avi\t\t\t\t     Test_5.avi\n"," 28.avi   5.avi\t\t\t\t\t    'Test5 - NN.png'\n"," 29.avi   6.avi\t\t\t\t\t     Test_video\n"," 2.avi\t  7.avi\t\t\t\t\t    'timeLabel_test10 NN.json'\n"," 30.avi   8.avi\t\t\t\t\t    'timeLabel_test10 TL.json'\n"," 31.avi   9.avi\t\t\t\t\t    'timeLabel_test1 NN.json'\n"," 32.avi   best_model\t\t\t\t    'timeLabel_test2 NN.json'\n"," 33.avi   combinedlabels2.csv\t\t\t    'timeLabel_test3 NN.json'\n"," 34.avi   Data2.npy\t\t\t\t    'timeLabel_test4 NN.json'\n"," 35.avi   Data.npy\t\t\t\t    'timeLabel_test5 NN.json'\n"," 36.avi  'Demo for video'\t\t\t     Train\n"," 37.avi   Drowsiness-Detection-using-Deep-Learning   Untitled0.ipynb\n"," 38.avi   Features_final2.csv\t\t\t     vgg16_1.h5\n"," 39.avi   Features_final.csv\t\t\t     vgg16_2.h5\n"," 3.avi\t 'First submission'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"ESpo1bSQhIYF","colab_type":"code","outputId":"de7776f5-0cd4-43f3-901d-e8d3b07aad24","executionInfo":{"status":"ok","timestamp":1585709477825,"user_tz":300,"elapsed":798,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["cd '/content/drive/My Drive/CSCE_689/'"],"execution_count":8,"outputs":[{"output_type":"stream","text":["/content/drive/My Drive/CSCE_689\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NnUFdifGbdRI","colab_type":"text"},"source":["### Define and compute inputs terms "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s-p-3knNupj6","colab":{}},"source":["def eye_aspect_ratio(eye):\n","\t\n","\tA = distance.euclidean(eye[1], eye[5])\n","\tB = distance.euclidean(eye[2], eye[4])\n","\tC = distance.euclidean(eye[0], eye[3])\n","\tear = (A + B) / (2.0 * C)\n","\treturn ear"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Gacb2hPmubbr","colab":{}},"source":["def mouth_aspect_ratio(mouth):\n","    A = distance.euclidean(mouth[14], mouth[18])\n","    C = distance.euclidean(mouth[12], mouth[16])\n","    mar = (A ) / (C)\n","    return mar"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"dYhTecWBuygh","colab":{}},"source":["def circularity(eye):\n","    A = distance.euclidean(eye[1], eye[4])\n","    radius  = A/2.0\n","    Area = math.pi * (radius ** 2)\n","    p = 0\n","    p += distance.euclidean(eye[0], eye[1])\n","    p += distance.euclidean(eye[1], eye[2])\n","    p += distance.euclidean(eye[2], eye[3])\n","    p += distance.euclidean(eye[3], eye[4])\n","    p += distance.euclidean(eye[4], eye[5])\n","    p += distance.euclidean(eye[5], eye[0])\n","    return 4 * math.pi * Area /(p**2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"0FpZ1jDou4dx","colab":{}},"source":["def mouth_over_eye(eye):\n","    ear = eye_aspect_ratio(eye)\n","    mar = mouth_aspect_ratio(eye)\n","    mouth_eye = mar/ear\n","    return mouth_eye"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"f0T0PX03fPGu","colab_type":"code","colab":{}},"source":["def jaw_dist(jaw,nose):\n","  list= np.array([distance.euclidean(d, nose) for d in jaw])\n","  return list.max()/list.mean()\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"aZ0uQXMcu81N","colab":{}},"source":["def getFrame(sec):\n","    start = 0\n","    vidcap.set(cv2.CAP_PROP_POS_MSEC, start + sec*1000)\n","    hasFrames,image = vidcap.read()\n","    return hasFrames, image"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeCoh3aWY2Kc","colab_type":"code","colab":{}},"source":["def get_landmarks(vidcap,i):\n","  sec = 0\n","  frameRate = 0.5\n","  success, image = getFrame(sec)\n","  count = 0\n","  data=[]\n","  labels=[]\n","  timestamp=[]\n","  while success and count < int(vidcap.get(cv2.CAP_PROP_POS_FRAMES)):\n","      landmarks = extract_face_landmarks(image)\n","      try:\n","          sum(sum(landmarks))\n","          if sum(sum(landmarks)) != 0:\n","              count += 1\n","              data.append(landmarks)\n","              labels.append([i])\n","              # cv2.imwrite(\"frame%d.jpg\" % count, image)\n","              timestamp.append(sec)\n","              sec = sec + frameRate\n","              sec = round(sec, 3)\n","              success, image = getFrame(sec)\n","\n","              # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          else:\n","              sec = sec + frameRate\n","              sec = round(sec, 3)\n","              success, image = getFrame(sec)\n","              print(str(i)+' not detected')\n","      except:\n","          # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          count=int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))\n","          pass\n","      # print(count)\n","  return data,labels,timestamp"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tO2j7gMPu_3f","colab":{}},"source":["# Generate Landmarks for all 41 training videos\n","from scipy.spatial import distance\n","import cv2\n","data = []\n","labels = []\n","frmes=[]\n","timestamp=[]\n","for i in range(59):\n","    vidcap = cv2.VideoCapture(str(i) + '.avi')\n","    data.extend(get_landmarks(vidcap,i)[0])\n","    labels.extend(get_landmarks(vidcap,i)[1])\n","    timestamp.extend(get_landmarks(vidcap,i)[2])\n","   \n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dJ2wTjNQvqPd","colab_type":"code","colab":{}},"source":["##Create Features\n","import numpy as np\n","import math\n","data = np.array(data)\n","features = []\n","main_list=[]\n","jaw_list=[]\n","for d in data:\n","    eye = d[36:68]\n","    nose=d[28]\n","    jaw=d[5:13]\n","    main_list.append(eye)\n","    ear = eye_aspect_ratio(eye)\n","    mar = mouth_aspect_ratio(eye)\n","    cir = circularity(eye)\n","    movt = jaw_dist(jaw,nose)\n","    mouth_eye = mouth_over_eye(eye)\n","    features.append([ear, mar, cir, mouth_eye,movt])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"-9N4tUy9wu4z","colab":{}},"source":["import pandas as pd\n","features = np.array(features)\n","features.shape\n","time= np.array(timestamp)\n","x=pd.DataFrame()\n","x['label']=labels\n","x['timestamp']=time"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UKWDJ5-I0Pt7","colab":{}},"source":["# Save files\n","np.save(open('Data2.npy', 'wb'),data) ## Save landmark data\n","np.savetxt('Timestamps.csv',time)\n","np.savetxt(\"Features_final.csv\", features, delimiter = \",\")\n","np.savetxt(\"Labels.csv\", labels, delimiter = \",\")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wkrVm2NRY9Md","colab_type":"code","colab":{}},"source":["## Save a combine file of label and timestamp. It will be converted to Labels_final.csv which will have labels of 0 (for not yawning) and 1 (for yawning)\n","x.to_csv(\"combinedlabels.csv\") "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uZtPlC0J2KdH","colab":{}},"source":["# Loading the files \n","import numpy as np\n","import pandas as pd\n","labels=pd.read_csv('Labels_final.csv',header=None)\n","features=pd.read_csv('Features_final.csv',header=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_Sa5W48KGcRr","colab_type":"code","outputId":"96e967e9-b271-4175-ce1d-a92085f72424","executionInfo":{"status":"ok","timestamp":1585710619910,"user_tz":300,"elapsed":1112831,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}},"colab":{"base_uri":"https://localhost:8080/","height":935}},"source":["## Generating train images for transfer learning\n","import cv2\n","j=0\n","c_a=0\n","c_b=0\n","for i in range(52):\n","  vidcap = cv2.VideoCapture(str(i) + '.avi')\n","  print(i)\n","  sec = 0\n","  frameRate = 0.5\n","  success, image = getFrame(sec)\n","  count = 0\n","  while success and count <= int(vidcap.get(cv2.CAP_PROP_POS_FRAMES)):\n","      landmarks = extract_face_landmarks(image)\n","      try:\n","          sum(sum(landmarks))\n","          if sum(sum(landmarks)) != 0:\n","            count += 1\n","            \n","            if labels.iloc[j,2]==0:\n","              # print('0')\n","              c_a=c_a+1\n","              cv2.imwrite('/content/drive/My Drive/CSCE_689/Train/Class_0/'+format(c_a)+str(\".jpg\"), image)\n","            else:\n","              # print('1')\n","              c_b=c_b+1\n","              cv2.imwrite('/content/drive/My Drive/CSCE_689/Train/Class_1/'+format(c_b)+str(\".jpg\"), image)\n","            sec = sec + frameRate\n","            sec = round(sec, 3)\n","            success, image = getFrame(sec)\n","            j=j+1\n","            # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          else:\n","            sec = sec + frameRate\n","            sec = round(sec, 3)\n","            success, image = getFrame(sec)\n","            # print(str(i)+' not detected')\n","      except:\n","          # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          count=int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))\n","          pass\n","      # print(count)"],"execution_count":17,"outputs":[{"output_type":"stream","text":["0\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/mlxtend/image/extract_face_landmarks.py:61: UserWarning: No face detected.\n","  warnings.warn('No face detected.')\n"],"name":"stderr"},{"output_type":"stream","text":["1\n","2\n","3\n","4\n","5\n","6\n","7\n","8\n","9\n","10\n","11\n","12\n","13\n","14\n","15\n","16\n","17\n","18\n","19\n","20\n","21\n","22\n","23\n","24\n","25\n","26\n","27\n","28\n","29\n","30\n","31\n","32\n","33\n","34\n","35\n","36\n","37\n","38\n","39\n","40\n","41\n","42\n","43\n","44\n","45\n","46\n","47\n","48\n","49\n","50\n","51\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p2qY9zNWcc_u","colab_type":"code","outputId":"aec4503a-a862-4594-ae8f-137228191d45","executionInfo":{"status":"ok","timestamp":1585639349187,"user_tz":300,"elapsed":287945,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["## Generating validation images for transfer learning\n","import cv2\n","j=4014\n","c_a=0\n","c_b=0\n","for i in range(52,59):\n","  vidcap = cv2.VideoCapture(str(i) + '.avi')\n","  sec = 0\n","  frameRate = 0.5\n","  success, image = getFrame(sec)\n","  count = 0\n","  while success and count < int(vidcap.get(cv2.CAP_PROP_POS_FRAMES)):\n","      landmarks = extract_face_landmarks(image)\n","      try:\n","          sum(sum(landmarks))\n","          if sum(sum(landmarks)) != 0:\n","            count += 1\n","            j=j+1\n","            if labels.iloc[j,2]==0:\n","              # print('0')\n","              c_a=c_a+1\n","              cv2.imwrite('/content/drive/My Drive/CSCE_689/Test/Class_0/'+format(c_a)+str(\".jpg\"), image)\n","            else:\n","              # print('1')\n","              c_b=c_b+1\n","              cv2.imwrite('/content/drive/My Drive/CSCE_689/Test/Class_1/'+format(c_b)+str(\".jpg\"), image)\n","            sec = sec + frameRate\n","            sec = round(sec, 3)\n","            success, image = getFrame(sec)\n","            \n","            # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          else:\n","            sec = sec + frameRate\n","            sec = round(sec, 3)\n","            success, image = getFrame(sec)\n","            # print(str(i)+' not detected')\n","      except:\n","          # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          count=int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))\n","          pass\n","      # print(count)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/mlxtend/image/extract_face_landmarks.py:61: UserWarning: No face detected.\n","  warnings.warn('No face detected.')\n"],"name":"stderr"}]},{"cell_type":"code","metadata":{"id":"bSL3LntbT15E","colab_type":"code","outputId":"13769420-98ca-49ff-c8a2-2defd7d16aa6","executionInfo":{"status":"ok","timestamp":1585688521123,"user_tz":300,"elapsed":226558,"user":{"displayName":"Sohil Parsana","photoUrl":"","userId":"08979242035941750356"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["j=4014\n","import cv2\n","for i in range(52,59):\n","  vidcap = cv2.VideoCapture(str(i) + '.avi')\n","  sec = 0\n","  frameRate = 0.5\n","  success, image = getFrame(sec)\n","  count = 0\n","  while success and count < int(vidcap.get(cv2.CAP_PROP_POS_FRAMES)):\n","      landmarks = extract_face_landmarks(image)\n","      try:\n","          sum(sum(landmarks))\n","          if sum(sum(landmarks)) != 0:\n","            count += 1\n","            j=j+1\n","            cv2.imwrite('/content/drive/My Drive/CSCE_689/Prediction/'+format(j)+str(\".jpg\"), image)\n","            sec = sec + frameRate\n","            sec = round(sec, 3)\n","            success, image = getFrame(sec)\n","            \n","            # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          else:\n","            sec = sec + frameRate\n","            sec = round(sec, 3)\n","            success, image = getFrame(sec)\n","            # print(str(i)+' not detected')\n","      except:\n","          # print('success: '+ str(success)+str(int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))))\n","          count=int(vidcap.get(cv2.CAP_PROP_POS_FRAMES))\n","          pass"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/mlxtend/image/extract_face_landmarks.py:66: UserWarning: No face detected.\n","  warnings.warn('No face detected.')\n"],"name":"stderr"}]}]}